{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.load(f'../fashion_train.npy')\n",
    "test_data = np.load(f'../fashion_test.npy')\n",
    "\n",
    "labels = training_data[:, -1]\n",
    "training_data = training_data[:,:-1] / 255 # Normalise pixel values\n",
    "training_data = np.c_[training_data, labels] # Add labels back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def MSE(y_pred, y):\n",
    "    return 2 * np.mean([(y_i - y_pred_i)**2 for y_i, y_pred_i in zip(y, y_pred)])\n",
    "\n",
    "def MSE_derivative(y_pred, y):\n",
    "    return [(y_i - y_pred_i) for y_i, y_pred_i in zip(y, y_pred)]\n",
    "\n",
    "def CrossEntropy(y_pred, y):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(np.sum(y * np.log(y_pred), axis=1))\n",
    "\n",
    "def CrossEntropy_derivative(y_pred, y):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return (y_pred - y) / y.shape[0]\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def reLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def reLU_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def leaky_reLU(x):\n",
    "    return np.maximum(0.01 * x, x)\n",
    "\n",
    "def leaky_reLU_derivative(x):\n",
    "    return np.where(x > 0, 1, 0.01)\n",
    "\n",
    "def ELU(x):\n",
    "    return np.maximum(0.01 * (np.exp(x)-1), x)\n",
    "\n",
    "def ELU_derivative(x):\n",
    "    return np.where(x > 0, 1, 0.01* np.exp(x))\n",
    "\n",
    "def softmax(x):\n",
    "    exp_values = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "    return probabilities\n",
    "\n",
    "def xavier_initialization(input_size, output_size):\n",
    "    return np.random.randn(input_size, output_size) * np.sqrt(1 / input_size)\n",
    "\n",
    "def he_initialization(input_size, output_size):\n",
    "    return np.random.randn(input_size, output_size) * np.sqrt(2 / input_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(data, weights, biases, num_hidden_layers, activation_function):\n",
    "    inputs, outputs = [], []\n",
    "    current_input = data\n",
    "\n",
    "    z = np.dot(current_input, weights[\"input\"]) + biases[\"input\"]\n",
    "    inputs.append(z)\n",
    "    current_input = activation_function(z)\n",
    "    outputs.append(current_input)\n",
    "\n",
    "    for i in range(num_hidden_layers):\n",
    "        z = np.dot(current_input, weights[\"hidden\"][i]) + biases[\"hidden\"][i]\n",
    "        inputs.append(z)\n",
    "        current_input = activation_function(z)\n",
    "        outputs.append(current_input)\n",
    "\n",
    "    z = np.dot(current_input, weights[\"output\"]) + biases[\"output\"]\n",
    "    inputs.append(z)\n",
    "    current_input = softmax(z)\n",
    "    outputs.append(current_input)\n",
    "\n",
    "    return inputs, outputs\n",
    "\n",
    "def backward_pass(data, labels, weights, biases, inputs, outputs, activation_function_derivative, cost_derivative, learning_rate):\n",
    "    # Calculate delta for output layer\n",
    "    delta = cost_derivative(outputs[-1], labels) * activation_function_derivative(inputs[-1])\n",
    "    \n",
    "    # Gradients for output layer\n",
    "    gradient_weights_output = np.dot(outputs[-2].T, delta)  # Last hidden layer's output\n",
    "    gradient_biases_output = np.sum(delta, axis=0, keepdims=True)\n",
    "\n",
    "    # Update output layer weights and biases\n",
    "    weights[\"output\"] -= learning_rate * gradient_weights_output\n",
    "    biases[\"output\"] -= learning_rate * gradient_biases_output\n",
    "\n",
    "    num_hidden_layers = len(weights[\"hidden\"])\n",
    "    # Gradients for hidden layers\n",
    "    for i in range(num_hidden_layers-1, -1, -1):\n",
    "        if i == len(weights[\"hidden\"])-1:\n",
    "            delta = np.dot(delta, weights[\"output\"].T) * activation_function_derivative(inputs[-2])\n",
    "        else:\n",
    "            delta = np.dot(delta, weights[\"hidden\"][i+1].T) * activation_function_derivative(inputs[-2 +  i - num_hidden_layers])\n",
    "\n",
    "        gradient_weights_hidden = np.dot(outputs[-2 +  i - num_hidden_layers].T, delta)\n",
    "        gradient_biases_hidden = np.sum(delta, axis=0, keepdims=True)\n",
    "        # Update hidden layer weights and biases\n",
    "        weights[\"hidden\"][i] -= learning_rate * gradient_weights_hidden\n",
    "        biases[\"hidden\"][i] -= learning_rate * gradient_biases_hidden\n",
    "\n",
    "    delta = np.dot(delta, weights[\"hidden\"][0].T) * activation_function_derivative(inputs[0])\n",
    "    gradient_weights_input = np.dot(data.T, delta)\n",
    "    gradient_biases_input = np.sum(delta, axis=0, keepdims=True)\n",
    "\n",
    "    # Update input layer weights and biases\n",
    "    weights[\"input\"] -= learning_rate * gradient_weights_input\n",
    "    biases[\"input\"] -= learning_rate * gradient_biases_input\n",
    "\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(data, num_hidden_layers, learning_rate, epochs, activation_function):\n",
    "    np.random.seed(42)\n",
    "\n",
    "    if activation_function == 'sigmoid':\n",
    "        initialization = xavier_initialization\n",
    "        activation_function_ = sigmoid\n",
    "        activation_function_derivative_ = sigmoid_derivative\n",
    "        cost_function_ = MSE\n",
    "        cost_derivative_ = MSE_derivative\n",
    "    elif activation_function == 'relu':\n",
    "        initialization = he_initialization\n",
    "        activation_function_ = reLU\n",
    "        activation_function_derivative_ = reLU_derivative\n",
    "        cost_function_ = CrossEntropy\n",
    "        cost_derivative_ = CrossEntropy_derivative\n",
    "    elif activation_function == 'leaky_relu':\n",
    "        initialization = he_initialization\n",
    "        activation_function_ = leaky_reLU\n",
    "        activation_function_derivative_ = leaky_reLU_derivative\n",
    "        cost_function_ = CrossEntropy\n",
    "        cost_derivative_ = CrossEntropy_derivative\n",
    "    elif activation_function == 'ELU':\n",
    "        initialization = he_initialization\n",
    "        activation_function_ = ELU\n",
    "        activation_function_derivative_ = ELU_derivative\n",
    "        cost_function_ = CrossEntropy\n",
    "        cost_derivative_ = CrossEntropy_derivative\n",
    "    else:\n",
    "        raise ValueError(\"Invalid activation function. Choose 'sigmoid', 'leaky_relu', 'ELU' or 'relu'.\")\n",
    "        \n",
    "    # Weight initialization\n",
    "    num_samples, num_features = data.shape[0], data.shape[1]-1\n",
    "    input_layer_size = num_features\n",
    "    hidden_layer_size = 16\n",
    "\n",
    "    output_layer_size = len(np.unique(data[:,-1]))\n",
    "\n",
    "    weights = {\n",
    "        \"input\": initialization(input_layer_size, hidden_layer_size),\n",
    "        \"hidden\": np.array([initialization(hidden_layer_size, hidden_layer_size) for _ in range(num_hidden_layers)]),\n",
    "        \"output\": initialization(hidden_layer_size, output_layer_size)\n",
    "    }\n",
    "    biases = {\n",
    "        \"input\": np.zeros((1, hidden_layer_size)),\n",
    "        \"hidden\": [np.zeros((1, hidden_layer_size)) for _ in range(num_hidden_layers)],\n",
    "        \"output\": np.zeros((1, output_layer_size))\n",
    "    }\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    labels = data[:, -1]\n",
    "    data_no_labels = data[:, :-1]\n",
    "    num_classes = len(np.unique(labels))\n",
    "    classes = np.eye(num_classes)[labels.astype(int)]\n",
    "\n",
    "    input, output = forward_pass(data_no_labels, weights, biases, num_hidden_layers,activation_function_)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        weights, biases = backward_pass(data_no_labels, classes, weights, biases, input, output, activation_function_derivative_, cost_derivative_, learning_rate)\n",
    "\n",
    "        input, output = forward_pass(data_no_labels, weights, biases, num_hidden_layers, activation_function_)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            cost = cost_function_(output[-1], classes)\n",
    "            print(f\"Epoch: {epoch}, Cost: {cost}\")\n",
    "    return weights, biases, num_hidden_layers, activation_function_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, weights, biases, num_hidden_layers, activation_function):\n",
    "    data_labels = data[:, -1].astype(int)\n",
    "    data_no_labels = data[:, :-1]\n",
    "    inputs, outputs = forward_pass(data_no_labels, weights, biases, num_hidden_layers, activation_function)\n",
    "    pred = np.argmax(outputs[-1], axis=1).astype(int)\n",
    "    accuracy = (np.sum(pred == data_labels) / len(data_labels)) * 100\n",
    "    num_classes = len(np.unique(data_labels))\n",
    "    confusion_matrix = np.zeros((num_classes, num_classes),dtype=int)\n",
    "    for i in range(len(data_labels)):\n",
    "        confusion_matrix[data_labels[i], pred[i]] += 1\n",
    "    return accuracy, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cost: 1.6027238676959559\n",
      "Epoch: 10, Cost: 1.602689328623306\n",
      "Epoch: 20, Cost: 1.5510086640732406\n",
      "Epoch: 30, Cost: 1.5017448087495124\n",
      "Epoch: 40, Cost: 1.4631836583146443\n",
      "Epoch: 50, Cost: 1.4318639062831764\n",
      "Epoch: 60, Cost: 1.418298946627136\n",
      "Epoch: 70, Cost: 1.41136436115077\n",
      "Epoch: 80, Cost: 1.398344956827646\n",
      "Epoch: 90, Cost: 1.3966992259402418\n",
      "Epoch: 100, Cost: 1.4088884762114515\n",
      "Epoch: 110, Cost: 1.4566718150215086\n",
      "Epoch: 120, Cost: 1.6249691055197517\n",
      "Epoch: 130, Cost: 1.6616098027800963\n",
      "Epoch: 140, Cost: 1.6647675697732014\n",
      "Epoch: 150, Cost: 1.6499859433113158\n",
      "Epoch: 160, Cost: 1.6342023991760743\n",
      "Epoch: 170, Cost: 1.6391462731068998\n",
      "Epoch: 180, Cost: 1.6258816024990193\n",
      "Epoch: 190, Cost: 1.6210660878684648\n",
      "Epoch: 200, Cost: 1.497468973308979\n",
      "Epoch: 210, Cost: 1.5383706827665289\n",
      "Epoch: 220, Cost: 1.4592161614262176\n",
      "Epoch: 230, Cost: 1.427747768695792\n",
      "Epoch: 240, Cost: 1.4380180990869373\n",
      "Epoch: 250, Cost: 1.330282506969516\n",
      "Epoch: 260, Cost: 1.3226630378154125\n",
      "Epoch: 270, Cost: 1.2048170287640012\n",
      "Epoch: 280, Cost: 1.270366027844206\n",
      "Epoch: 290, Cost: 1.1641275432147427\n",
      "Epoch: 300, Cost: 1.138545250410677\n",
      "Epoch: 310, Cost: 1.145295477907735\n",
      "Epoch: 320, Cost: 1.1118114204881637\n",
      "Epoch: 330, Cost: 1.1831412738160791\n",
      "Epoch: 340, Cost: 1.0784724261533996\n",
      "Epoch: 350, Cost: 1.1640206891029714\n",
      "Epoch: 360, Cost: 1.1258300688924754\n",
      "Epoch: 370, Cost: 1.1068145166471408\n",
      "Epoch: 380, Cost: 1.0713143006946138\n",
      "Epoch: 390, Cost: 1.0513808696768923\n",
      "Epoch: 400, Cost: 1.0348132101916525\n",
      "Epoch: 410, Cost: 1.01956795252678\n",
      "Epoch: 420, Cost: 1.0020199716330769\n",
      "Epoch: 430, Cost: 0.9878735241654316\n",
      "Epoch: 440, Cost: 0.9755882881677403\n",
      "Epoch: 450, Cost: 0.9662018128323377\n",
      "Epoch: 460, Cost: 0.9561893301629297\n",
      "Epoch: 470, Cost: 0.9472487966367351\n",
      "Epoch: 480, Cost: 0.9385018883070688\n",
      "Epoch: 490, Cost: 0.9319463632724561\n",
      "Epoch: 500, Cost: 0.9240902115848916\n",
      "Epoch: 510, Cost: 0.9152986327587564\n",
      "Epoch: 520, Cost: 0.9089611195721445\n",
      "Epoch: 530, Cost: 0.9037102068813972\n",
      "Epoch: 540, Cost: 0.8991626323279511\n",
      "Epoch: 550, Cost: 0.8941210917643544\n",
      "Epoch: 560, Cost: 0.8894649125324344\n",
      "Epoch: 570, Cost: 0.8845419713493151\n",
      "Epoch: 580, Cost: 0.8789959001949333\n",
      "Epoch: 590, Cost: 0.8744434344758076\n",
      "Epoch: 600, Cost: 0.8703216905177734\n",
      "Epoch: 610, Cost: 0.8657067870145784\n",
      "Epoch: 620, Cost: 0.8617022803553442\n",
      "Epoch: 630, Cost: 0.8591110755403253\n",
      "Epoch: 640, Cost: 0.8571689865280139\n",
      "Epoch: 650, Cost: 0.8577693971575527\n",
      "Epoch: 660, Cost: 0.8698540344762665\n",
      "Epoch: 670, Cost: 0.861572233831841\n",
      "Epoch: 680, Cost: 0.8611887506706906\n",
      "Epoch: 690, Cost: 0.8577243534280953\n",
      "Epoch: 700, Cost: 0.8543850380015668\n",
      "Epoch: 710, Cost: 0.8513613262308741\n",
      "Epoch: 720, Cost: 0.8492828726795196\n",
      "Epoch: 730, Cost: 0.8478211336633446\n",
      "Epoch: 740, Cost: 0.8443838957193509\n",
      "Epoch: 750, Cost: 0.8419342465415363\n",
      "Epoch: 760, Cost: 0.8382766451800667\n",
      "Epoch: 770, Cost: 0.8352305290676421\n",
      "Epoch: 780, Cost: 0.8328584616416141\n",
      "Epoch: 790, Cost: 0.8303845919137867\n",
      "Epoch: 800, Cost: 0.8267446176801246\n",
      "Epoch: 810, Cost: 0.8218049402926528\n",
      "Epoch: 820, Cost: 0.8196058329615171\n",
      "Epoch: 830, Cost: 0.8144451115985923\n",
      "Epoch: 840, Cost: 0.8085094037965871\n",
      "Epoch: 850, Cost: 0.8051899585511697\n",
      "Epoch: 860, Cost: 0.801158894221587\n",
      "Epoch: 870, Cost: 0.7971816385571893\n",
      "Epoch: 880, Cost: 0.7941850298848704\n",
      "Epoch: 890, Cost: 0.7919932517268318\n",
      "Epoch: 900, Cost: 0.7869119208846524\n",
      "Epoch: 910, Cost: 0.7828483524769195\n",
      "Epoch: 920, Cost: 0.7801176733281905\n",
      "Epoch: 930, Cost: 0.777368284745812\n",
      "Epoch: 940, Cost: 0.77536891534129\n",
      "Epoch: 950, Cost: 0.7722850789979071\n",
      "Epoch: 960, Cost: 0.7687317050227561\n",
      "Epoch: 970, Cost: 0.7663732400094699\n",
      "Epoch: 980, Cost: 0.762721930405492\n",
      "Epoch: 990, Cost: 0.7599244597493435\n"
     ]
    }
   ],
   "source": [
    "# currently 10k with 2 hidden layers and leaky relu takes 14-16 minutes (with my pc)\n",
    "weights, biases, num_hidden_layers, activation_function_ = feedforward(training_data, 2, 0.1, 1000, 'leaky_relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.46%\n",
      "[[1567   16   42  173  235]\n",
      " [ 113 1637   37  116   44]\n",
      " [  11    7 1537   24  422]\n",
      " [  68   21   17 1771  128]\n",
      " [ 340   18  377  145 1134]]\n"
     ]
    }
   ],
   "source": [
    "accuracy, confusion_matrix = predict(training_data, weights, biases, num_hidden_layers, activation_function_)\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Dense(50, activation='relu', input_shape=(10,)),  # 10 features\n",
    "    Dense(3, activation='softmax')  # 3 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Example usage\n",
    "# model.fit(train_data, train_labels, epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
