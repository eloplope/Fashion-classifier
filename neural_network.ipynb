{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3., 4.])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = np.load(f'../fashion_train.npy')\n",
    "test_data = np.load(f'../fashion_test.npy')\n",
    "\n",
    "labels = training_data[:, -1]\n",
    "training_data = training_data[:,:-1] / 255\n",
    "training_data = np.c_[training_data, labels]\n",
    "\n",
    "np.unique(training_data[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def MSE(y_pred, y):\n",
    "    return 2 * np.mean([(y_i - y_pred_i)**2 for y_i, y_pred_i in zip(y, y_pred)])\n",
    "\n",
    "def MSE_derivative(y_pred, y):\n",
    "    return [(y_i - y_pred_i) for y_i, y_pred_i in zip(y, y_pred)]\n",
    "\n",
    "def CrossEntropy(y_pred, y):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(np.sum(y * np.log(y_pred), axis=1))\n",
    "\n",
    "def CrossEntropy_derivative(y_pred, y):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return (y_pred - y) / y.shape[0]  # Vectorized\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def reLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def reLU_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def leaky_reLU(x):\n",
    "    return np.maximum(0.01 * x, x)\n",
    "\n",
    "def leaky_reLU_derivative(x):\n",
    "    return np.where(x > 0, 1, 0.01)\n",
    "\n",
    "def ELU(x):\n",
    "    return np.maximum(0.01 * (np.exp(x)-1), x)\n",
    "\n",
    "def ELU_derivative(x):\n",
    "    return np.where(x > 0, 1, 0.01* np.exp(x))\n",
    "\n",
    "def softmax(x):\n",
    "    exp_values = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "    return probabilities\n",
    "\n",
    "def xavier_initialization(input_size, output_size):\n",
    "    return np.random.randn(input_size, output_size) * np.sqrt(1 / input_size)\n",
    "\n",
    "def he_initialization(input_size, output_size):\n",
    "    return np.random.randn(input_size, output_size) * np.sqrt(2 / input_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(data, weights, biases, num_hidden_layers, activation_function):\n",
    "    inputs, outputs = [], []\n",
    "    current_input = data\n",
    "\n",
    "    z = np.dot(current_input, weights[\"input\"]) + biases[\"input\"]\n",
    "    inputs.append(z)\n",
    "    current_input = activation_function(z)\n",
    "    outputs.append(current_input)\n",
    "\n",
    "    for i in range(num_hidden_layers):\n",
    "        z = np.dot(current_input, weights[\"hidden\"][i]) + biases[\"hidden\"][i]\n",
    "        inputs.append(z)\n",
    "        current_input = activation_function(z)\n",
    "        outputs.append(current_input)\n",
    "\n",
    "    z = np.dot(current_input, weights[\"output\"]) + biases[\"output\"]\n",
    "    inputs.append(z)\n",
    "    current_input = softmax(z)\n",
    "    outputs.append(current_input)\n",
    "\n",
    "    return inputs, outputs\n",
    "\n",
    "def backward_pass(data, labels, weights, biases, inputs, outputs, activation_function_derivative, cost_derivative, learning_rate):\n",
    "    # Calculate delta for output layer\n",
    "    delta = cost_derivative(outputs[-1], labels) * activation_function_derivative(inputs[-1])\n",
    "\n",
    "    # Gradients for output layer\n",
    "    gradient_weights_output = np.dot(outputs[-2].T, delta)  # Last hidden layer's output\n",
    "    gradient_biases_output = np.sum(delta, axis=0, keepdims=True)\n",
    "\n",
    "    # Update output layer weights and biases\n",
    "    weights[\"output\"] -= learning_rate * gradient_weights_output\n",
    "    biases[\"output\"] -= learning_rate * gradient_biases_output\n",
    "\n",
    "    num_hidden_layers = len(weights[\"hidden\"])\n",
    "    # Gradients for hidden layers (backpropagate)\n",
    "    for i in range(num_hidden_layers-1, -1, -1):\n",
    "        if i == len(weights[\"hidden\"])-1:\n",
    "            delta = np.dot(delta, weights[\"output\"].T) * activation_function_derivative(inputs[-2])\n",
    "        else:\n",
    "            delta = np.dot(delta, weights[\"hidden\"][i+1].T) * activation_function_derivative(inputs[-2 +  i - num_hidden_layers])\n",
    "\n",
    "        gradient_weights_hidden = np.dot(outputs[-2 +  i - num_hidden_layers].T, delta)\n",
    "        gradient_biases_hidden = np.sum(delta, axis=0, keepdims=True)\n",
    "        # Update hidden layer weights and biases\n",
    "        weights[\"hidden\"][i] -= learning_rate * gradient_weights_hidden\n",
    "        biases[\"hidden\"][i] -= learning_rate * gradient_biases_hidden\n",
    "\n",
    "    delta = np.dot(delta, weights[\"hidden\"][0].T) * activation_function_derivative(inputs[0])\n",
    "    gradient_weights_input = np.dot(data.T, delta)\n",
    "    gradient_biases_input = np.sum(delta, axis=0, keepdims=True)\n",
    "\n",
    "    # Update input layer weights and biases\n",
    "    weights[\"input\"] -= learning_rate * gradient_weights_input\n",
    "    biases[\"input\"] -= learning_rate * gradient_biases_input\n",
    "\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(data, num_hidden_layers, learning_rate, epochs, activation_function):\n",
    "\n",
    "    # Select activation function\n",
    "    softmax_activation = softmax \n",
    "\n",
    "    if activation_function == 'sigmoid':\n",
    "        initialization = xavier_initialization\n",
    "        activation_function_ = sigmoid\n",
    "        activation_function_derivative_ = sigmoid_derivative\n",
    "        cost_function_ = MSE\n",
    "        cost_derivative_ = MSE_derivative\n",
    "    elif activation_function == 'relu':\n",
    "        initialization = he_initialization\n",
    "        activation_function_ = reLU\n",
    "        activation_function_derivative_ = reLU_derivative\n",
    "        cost_function_ = CrossEntropy\n",
    "        cost_derivative_ = CrossEntropy_derivative\n",
    "    elif activation_function == 'leaky_relu':\n",
    "        initialization = he_initialization\n",
    "        activation_function_ = leaky_reLU\n",
    "        activation_function_derivative_ = leaky_reLU_derivative\n",
    "        cost_function_ = CrossEntropy\n",
    "        cost_derivative_ = CrossEntropy_derivative\n",
    "    elif activation_function == 'ELU':\n",
    "        initialization = he_initialization\n",
    "        activation_function_ = ELU\n",
    "        activation_function_derivative_ = ELU_derivative\n",
    "        cost_function_ = CrossEntropy\n",
    "        cost_derivative_ = CrossEntropy_derivative\n",
    "    else:\n",
    "        raise ValueError(\"Invalid activation function. Choose 'sigmoid', 'leaky_relu', 'ELU' or 'relu'.\")\n",
    "        \n",
    "    # Weight initialization\n",
    "    num_samples, num_features = data.shape[0], data.shape[1]-1\n",
    "    input_layer_size = num_features\n",
    "    hidden_layer_size = 16\n",
    "    # num_features // num_hidden_layers\n",
    "\n",
    "    output_layer_size = len(np.unique(data[:,-1]))\n",
    "\n",
    "    weights = {\n",
    "        \"input\": initialization(input_layer_size, hidden_layer_size),\n",
    "        \"hidden\": np.array([initialization(hidden_layer_size, hidden_layer_size) for _ in range(num_hidden_layers)]),\n",
    "        \"output\": initialization(hidden_layer_size, output_layer_size)\n",
    "    }\n",
    "    biases = {\n",
    "        \"input\": np.zeros((1, hidden_layer_size)),\n",
    "        \"hidden\": [np.zeros((1, hidden_layer_size)) for _ in range(num_hidden_layers)],\n",
    "        \"output\": np.zeros((1, output_layer_size))\n",
    "    }\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    labels = data[:, -1]\n",
    "    data_no_labels = data[:, :-1]\n",
    "    num_classes = len(np.unique(labels))\n",
    "    classes = np.eye(num_classes)[labels.astype(int)]\n",
    "\n",
    "    input, output = forward_pass(data_no_labels, weights, biases, num_hidden_layers,activation_function_)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        weights, biases = backward_pass(data_no_labels, classes, weights, biases, input, output, activation_function_derivative_, cost_derivative_, learning_rate)\n",
    "\n",
    "        input, output = forward_pass(data_no_labels, weights, biases, num_hidden_layers, activation_function_)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            cost = cost_function_(output[-1], classes)\n",
    "            print(f\"Epoch: {epoch+1}, Cost: {cost}\")\n",
    "    return weights, biases, num_hidden_layers, activation_function_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, weights, biases, num_hidden_layers, activation_function):\n",
    "    data_labels = data[:, -1]\n",
    "    data_no_labels = data[:, :-1]\n",
    "    inputs, outputs = forward_pass(data_no_labels, weights, biases, num_hidden_layers, activation_function)\n",
    "    pred = np.argmax(outputs[-1], axis=1)\n",
    "    accuracy = (np.sum(pred == data_labels) / len(data_labels)) * 100\n",
    "    confusion_matrix = np.zeros((len(np.unique(data_labels)), len(np.unique(data_labels))))\n",
    "    for i in range(len(data_labels)):\n",
    "        confusion_matrix[data_labels[i]][pred[i]] += 1\n",
    "    return accuracy, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Cost: 1.6314219794817661\n",
      "Epoch: 11, Cost: 1.617173046212607\n",
      "Epoch: 21, Cost: 1.6164127395640224\n",
      "Epoch: 31, Cost: 1.5918563242027293\n",
      "Epoch: 41, Cost: 1.5590719249777707\n",
      "Epoch: 51, Cost: 1.5255854275951402\n",
      "Epoch: 61, Cost: 1.493439502102155\n",
      "Epoch: 71, Cost: 1.463738931417949\n",
      "Epoch: 81, Cost: 1.431439395716156\n",
      "Epoch: 91, Cost: 1.3970394130532375\n",
      "Epoch: 101, Cost: 1.35652503604993\n",
      "Epoch: 111, Cost: 1.3138590499982052\n",
      "Epoch: 121, Cost: 1.274496861645833\n",
      "Epoch: 131, Cost: 1.2393117494016077\n",
      "Epoch: 141, Cost: 1.2082263033170744\n",
      "Epoch: 151, Cost: 1.1798671969760577\n",
      "Epoch: 161, Cost: 1.1535376265465245\n",
      "Epoch: 171, Cost: 1.128806974276146\n",
      "Epoch: 181, Cost: 1.1051927616898185\n",
      "Epoch: 191, Cost: 1.082729948479877\n",
      "Epoch: 201, Cost: 1.0618171833619798\n",
      "Epoch: 211, Cost: 1.0422939874575898\n",
      "Epoch: 221, Cost: 1.0242874543849168\n",
      "Epoch: 231, Cost: 1.0077272324556419\n",
      "Epoch: 241, Cost: 0.9920450607359006\n",
      "Epoch: 251, Cost: 0.9769865454731844\n",
      "Epoch: 261, Cost: 0.9622039851425638\n",
      "Epoch: 271, Cost: 0.9472955374052515\n",
      "Epoch: 281, Cost: 0.9320272243423792\n",
      "Epoch: 291, Cost: 0.9162278319761541\n",
      "Epoch: 301, Cost: 0.9001398706566837\n",
      "Epoch: 311, Cost: 0.8837993061152016\n",
      "Epoch: 321, Cost: 0.8678039137747581\n",
      "Epoch: 331, Cost: 0.8528399900533992\n",
      "Epoch: 341, Cost: 0.8390958940469634\n",
      "Epoch: 351, Cost: 0.8267123691090432\n",
      "Epoch: 361, Cost: 0.8155804332576748\n",
      "Epoch: 371, Cost: 0.8054479198616694\n",
      "Epoch: 381, Cost: 0.7961285218483665\n",
      "Epoch: 391, Cost: 0.7874735807375212\n",
      "Epoch: 401, Cost: 0.7793371101897834\n",
      "Epoch: 411, Cost: 0.7715522962301423\n",
      "Epoch: 421, Cost: 0.7640618628490089\n",
      "Epoch: 431, Cost: 0.7567220798610703\n",
      "Epoch: 441, Cost: 0.7496852600329875\n",
      "Epoch: 451, Cost: 0.7428739133339236\n",
      "Epoch: 461, Cost: 0.7361858957592747\n",
      "Epoch: 471, Cost: 0.7296118145238395\n",
      "Epoch: 481, Cost: 0.723150246773301\n",
      "Epoch: 491, Cost: 0.7168684905595759\n",
      "Epoch: 501, Cost: 0.710765505972547\n",
      "Epoch: 511, Cost: 0.7050156659207987\n",
      "Epoch: 521, Cost: 0.6994954829539531\n",
      "Epoch: 531, Cost: 0.6942265011756105\n",
      "Epoch: 541, Cost: 0.689202027533716\n",
      "Epoch: 551, Cost: 0.6844296683792495\n",
      "Epoch: 561, Cost: 0.6798304847047988\n",
      "Epoch: 571, Cost: 0.6754748573216922\n",
      "Epoch: 581, Cost: 0.6712446369210668\n",
      "Epoch: 591, Cost: 0.6671131795867392\n",
      "Epoch: 601, Cost: 0.663222719724363\n",
      "Epoch: 611, Cost: 0.6595189551018305\n",
      "Epoch: 621, Cost: 0.655916477749184\n",
      "Epoch: 631, Cost: 0.652462276015988\n",
      "Epoch: 641, Cost: 0.6491585914941774\n",
      "Epoch: 651, Cost: 0.6459943312940676\n",
      "Epoch: 661, Cost: 0.6429425148713981\n",
      "Epoch: 671, Cost: 0.6399750026944174\n",
      "Epoch: 681, Cost: 0.6371152737852577\n",
      "Epoch: 691, Cost: 0.6343864073342902\n",
      "Epoch: 701, Cost: 0.6317477784009766\n",
      "Epoch: 711, Cost: 0.629246749811749\n",
      "Epoch: 721, Cost: 0.6268483141595782\n",
      "Epoch: 731, Cost: 0.6245232981297479\n",
      "Epoch: 741, Cost: 0.6222588265332925\n",
      "Epoch: 751, Cost: 0.6200922838054262\n",
      "Epoch: 761, Cost: 0.617995506278342\n",
      "Epoch: 771, Cost: 0.6159551404504165\n",
      "Epoch: 781, Cost: 0.6139753033295381\n",
      "Epoch: 791, Cost: 0.6120729592861732\n",
      "Epoch: 801, Cost: 0.6102248777985897\n",
      "Epoch: 811, Cost: 0.6084477462315003\n",
      "Epoch: 821, Cost: 0.6067179772945943\n",
      "Epoch: 831, Cost: 0.6050451317547865\n",
      "Epoch: 841, Cost: 0.6034279268266449\n",
      "Epoch: 851, Cost: 0.601841578541172\n",
      "Epoch: 861, Cost: 0.6003015144296565\n",
      "Epoch: 871, Cost: 0.5987934749874133\n",
      "Epoch: 881, Cost: 0.5973332362390829\n",
      "Epoch: 891, Cost: 0.5959041863616833\n",
      "Epoch: 901, Cost: 0.5945110158304074\n",
      "Epoch: 911, Cost: 0.5931435971827023\n",
      "Epoch: 921, Cost: 0.5918023970257863\n",
      "Epoch: 931, Cost: 0.5904922009392636\n",
      "Epoch: 941, Cost: 0.5892026842995589\n",
      "Epoch: 951, Cost: 0.5879274819945982\n",
      "Epoch: 961, Cost: 0.5866789105274913\n",
      "Epoch: 971, Cost: 0.5854517355442291\n",
      "Epoch: 981, Cost: 0.5842331168449406\n",
      "Epoch: 991, Cost: 0.5830433662957832\n"
     ]
    }
   ],
   "source": [
    "weights, biases, num_hidden_layers, activation_function_ = feedforward(training_data, 2, 0.005, 1000, 'leaky_relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77.12,\n",
       " array([[762.,   4.,  26.,  84., 124.],\n",
       "        [  3., 927.,  17.,  47.,   6.],\n",
       "        [  8.,  10., 818.,  15., 149.],\n",
       "        [ 48.,  20.,   9., 858.,  65.],\n",
       "        [192.,   3., 266.,  48., 491.]]))"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(test_data, weights, biases, num_hidden_layers, activation_function_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
