{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.load(f'../fashion_train.npy')\n",
    "test_data = np.load(f'../fashion_test.npy')\n",
    "\n",
    "labels = training_data[:, -1]\n",
    "training_data = training_data[:,:-1] / 255 # Normalise pixel values\n",
    "training_data = np.c_[training_data, labels] # Add labels back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def MSE(y_pred, y):\n",
    "    return 2 * np.mean([(y_i - y_pred_i)**2 for y_i, y_pred_i in zip(y, y_pred)])\n",
    "\n",
    "def MSE_derivative(y_pred, y):\n",
    "    return [(y_i - y_pred_i) for y_i, y_pred_i in zip(y, y_pred)]\n",
    "\n",
    "def CrossEntropy(y_pred, y):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(np.sum(y * np.log(y_pred), axis=1))\n",
    "\n",
    "def CrossEntropy_derivative(y_pred, y):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return (y_pred - y) / y.shape[0]\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def reLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def reLU_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def leaky_reLU(x):\n",
    "    return np.maximum(0.01 * x, x)\n",
    "\n",
    "def leaky_reLU_derivative(x):\n",
    "    return np.where(x > 0, 1, 0.01)\n",
    "\n",
    "def ELU(x):\n",
    "    return np.maximum(0.01 * (np.exp(x)-1), x)\n",
    "\n",
    "def ELU_derivative(x):\n",
    "    return np.where(x > 0, 1, 0.01* np.exp(x))\n",
    "\n",
    "def softmax(x):\n",
    "    exp_values = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "    return probabilities\n",
    "\n",
    "def xavier_initialization(input_size, output_size):\n",
    "    return np.random.randn(input_size, output_size) * np.sqrt(1 / input_size)\n",
    "\n",
    "def he_initialization(input_size, output_size):\n",
    "    return np.random.randn(input_size, output_size) * np.sqrt(2 / input_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(data, weights, biases, num_hidden_layers, activation_function):\n",
    "    inputs, outputs = [], []\n",
    "    current_input = data\n",
    "\n",
    "    z = np.dot(current_input, weights[\"input\"]) + biases[\"input\"]\n",
    "    inputs.append(z)\n",
    "    current_input = activation_function(z)\n",
    "    outputs.append(current_input)\n",
    "\n",
    "    for i in range(num_hidden_layers):\n",
    "        z = np.dot(current_input, weights[\"hidden\"][i]) + biases[\"hidden\"][i]\n",
    "        inputs.append(z)\n",
    "        current_input = activation_function(z)\n",
    "        outputs.append(current_input)\n",
    "\n",
    "    z = np.dot(current_input, weights[\"output\"]) + biases[\"output\"]\n",
    "    inputs.append(z)\n",
    "    current_input = softmax(z)\n",
    "    outputs.append(current_input)\n",
    "\n",
    "    return inputs, outputs\n",
    "\n",
    "def backward_pass(data, labels, weights, biases, inputs, outputs, activation_function_derivative, cost_derivative, learning_rate):\n",
    "    # Calculate delta for output layer\n",
    "    delta = cost_derivative(outputs[-1], labels) * activation_function_derivative(inputs[-1])\n",
    "    \n",
    "    # Gradients for output layer\n",
    "    gradient_weights_output = np.dot(outputs[-2].T, delta)  # Last hidden layer's output\n",
    "    gradient_biases_output = np.sum(delta, axis=0, keepdims=True)\n",
    "\n",
    "    # Update output layer weights and biases\n",
    "    weights[\"output\"] -= learning_rate * gradient_weights_output\n",
    "    biases[\"output\"] -= learning_rate * gradient_biases_output\n",
    "\n",
    "    num_hidden_layers = len(weights[\"hidden\"])\n",
    "    # Gradients for hidden layers\n",
    "    for i in range(num_hidden_layers-1, -1, -1):\n",
    "        if i == len(weights[\"hidden\"])-1:\n",
    "            delta = np.dot(delta, weights[\"output\"].T) * activation_function_derivative(inputs[-2])\n",
    "        else:\n",
    "            delta = np.dot(delta, weights[\"hidden\"][i+1].T) * activation_function_derivative(inputs[-2 +  i - num_hidden_layers])\n",
    "\n",
    "        gradient_weights_hidden = np.dot(outputs[-2 +  i - num_hidden_layers].T, delta)\n",
    "        gradient_biases_hidden = np.sum(delta, axis=0, keepdims=True)\n",
    "        # Update hidden layer weights and biases\n",
    "        weights[\"hidden\"][i] -= learning_rate * gradient_weights_hidden\n",
    "        biases[\"hidden\"][i] -= learning_rate * gradient_biases_hidden\n",
    "\n",
    "    delta = np.dot(delta, weights[\"hidden\"][0].T) * activation_function_derivative(inputs[0])\n",
    "    gradient_weights_input = np.dot(data.T, delta)\n",
    "    gradient_biases_input = np.sum(delta, axis=0, keepdims=True)\n",
    "\n",
    "    # Update input layer weights and biases\n",
    "    weights[\"input\"] -= learning_rate * gradient_weights_input\n",
    "    biases[\"input\"] -= learning_rate * gradient_biases_input\n",
    "\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(data, num_hidden_layers, learning_rate, epochs, activation_function):\n",
    "    np.random.seed(42)\n",
    "\n",
    "    if activation_function == 'sigmoid':\n",
    "        initialization = xavier_initialization\n",
    "        activation_function_ = sigmoid\n",
    "        activation_function_derivative_ = sigmoid_derivative\n",
    "        cost_function_ = MSE\n",
    "        cost_derivative_ = MSE_derivative\n",
    "    elif activation_function == 'relu':\n",
    "        initialization = he_initialization\n",
    "        activation_function_ = reLU\n",
    "        activation_function_derivative_ = reLU_derivative\n",
    "        cost_function_ = CrossEntropy\n",
    "        cost_derivative_ = CrossEntropy_derivative\n",
    "    elif activation_function == 'leaky_relu':\n",
    "        initialization = he_initialization\n",
    "        activation_function_ = leaky_reLU\n",
    "        activation_function_derivative_ = leaky_reLU_derivative\n",
    "        cost_function_ = CrossEntropy\n",
    "        cost_derivative_ = CrossEntropy_derivative\n",
    "    elif activation_function == 'ELU':\n",
    "        initialization = he_initialization\n",
    "        activation_function_ = ELU\n",
    "        activation_function_derivative_ = ELU_derivative\n",
    "        cost_function_ = CrossEntropy\n",
    "        cost_derivative_ = CrossEntropy_derivative\n",
    "    else:\n",
    "        raise ValueError(\"Invalid activation function. Choose 'sigmoid', 'leaky_relu', 'ELU' or 'relu'.\")\n",
    "        \n",
    "    # Weight initialization\n",
    "    num_samples, num_features = data.shape[0], data.shape[1]-1\n",
    "    input_layer_size = num_features\n",
    "    hidden_layer_size = 16\n",
    "\n",
    "    output_layer_size = len(np.unique(data[:,-1]))\n",
    "\n",
    "    weights = {\n",
    "        \"input\": initialization(input_layer_size, hidden_layer_size),\n",
    "        \"hidden\": np.array([initialization(hidden_layer_size, hidden_layer_size) for _ in range(num_hidden_layers)]),\n",
    "        \"output\": initialization(hidden_layer_size, output_layer_size)\n",
    "    }\n",
    "    biases = {\n",
    "        \"input\": np.zeros((1, hidden_layer_size)),\n",
    "        \"hidden\": [np.zeros((1, hidden_layer_size)) for _ in range(num_hidden_layers)],\n",
    "        \"output\": np.zeros((1, output_layer_size))\n",
    "    }\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    labels = data[:, -1]\n",
    "    data_no_labels = data[:, :-1]\n",
    "    num_classes = len(np.unique(labels))\n",
    "    classes = np.eye(num_classes)[labels.astype(int)]\n",
    "\n",
    "    input, output = forward_pass(data_no_labels, weights, biases, num_hidden_layers,activation_function_)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        weights, biases = backward_pass(data_no_labels, classes, weights, biases, input, output, activation_function_derivative_, cost_derivative_, learning_rate)\n",
    "\n",
    "        input, output = forward_pass(data_no_labels, weights, biases, num_hidden_layers, activation_function_)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            cost = cost_function_(output[-1], classes)\n",
    "            print(f\"Epoch: {epoch}, Cost: {cost}\")\n",
    "    return weights, biases, num_hidden_layers, activation_function_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, weights, biases, num_hidden_layers, activation_function):\n",
    "    data_labels = data[:, -1].astype(int)\n",
    "    data_no_labels = data[:, :-1]\n",
    "    inputs, outputs = forward_pass(data_no_labels, weights, biases, num_hidden_layers, activation_function)\n",
    "    pred = np.argmax(outputs[-1], axis=1).astype(int)\n",
    "    accuracy = (np.sum(pred == data_labels) / len(data_labels)) * 100\n",
    "    num_classes = len(np.unique(data_labels))\n",
    "    confusion_matrix = np.zeros((num_classes, num_classes),dtype=int)\n",
    "    for i in range(len(data_labels)):\n",
    "        confusion_matrix[data_labels[i], pred[i]] += 1\n",
    "    \n",
    "    misclassification_rate = []\n",
    "    for i in range(num_classes):\n",
    "        total_row = np.sum(confusion_matrix[i,:])\n",
    "        error_rate = confusion_matrix[i,i] / total_row\n",
    "        misclassification_rate.append(1-error_rate)\n",
    "    print(misclassification_rate)\n",
    "    return accuracy, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cost: 1.7869268404926895\n",
      "Epoch: 10, Cost: 1.6312338444039276\n",
      "Epoch: 20, Cost: 1.6123941365790984\n",
      "Epoch: 30, Cost: 1.6058320008857812\n",
      "Epoch: 40, Cost: 1.6032394194583999\n",
      "Epoch: 50, Cost: 1.6060213545430704\n",
      "Epoch: 60, Cost: 1.6090130236531268\n",
      "Epoch: 70, Cost: 1.6093960605759519\n",
      "Epoch: 80, Cost: 1.6084335255721383\n",
      "Epoch: 90, Cost: 1.6068094335989695\n",
      "Epoch: 100, Cost: 1.6044054419437574\n",
      "Epoch: 110, Cost: 1.60152251350777\n",
      "Epoch: 120, Cost: 1.5980826067995222\n",
      "Epoch: 130, Cost: 1.5941845277149265\n",
      "Epoch: 140, Cost: 1.5900119407622917\n",
      "Epoch: 150, Cost: 1.5857860421003893\n",
      "Epoch: 160, Cost: 1.5815521518630393\n",
      "Epoch: 170, Cost: 1.5770837919887148\n",
      "Epoch: 180, Cost: 1.572452614190054\n",
      "Epoch: 190, Cost: 1.5676946327418326\n",
      "Epoch: 200, Cost: 1.562749978229103\n",
      "Epoch: 210, Cost: 1.557640348416515\n",
      "Epoch: 220, Cost: 1.5520759427880453\n",
      "Epoch: 230, Cost: 1.5465612909649804\n",
      "Epoch: 240, Cost: 1.540986913848468\n",
      "Epoch: 250, Cost: 1.535275636451737\n",
      "Epoch: 260, Cost: 1.5294081156644201\n",
      "Epoch: 270, Cost: 1.5236634860218052\n",
      "Epoch: 280, Cost: 1.5180913412983204\n",
      "Epoch: 290, Cost: 1.5129413675750427\n",
      "Epoch: 300, Cost: 1.508215619076248\n",
      "Epoch: 310, Cost: 1.5039865719238652\n",
      "Epoch: 320, Cost: 1.5001504170382327\n",
      "Epoch: 330, Cost: 1.4967409059677255\n",
      "Epoch: 340, Cost: 1.493806199153841\n",
      "Epoch: 350, Cost: 1.4912709874332826\n",
      "Epoch: 360, Cost: 1.4891061452641527\n",
      "Epoch: 370, Cost: 1.4873731215368924\n",
      "Epoch: 380, Cost: 1.4857965228777663\n",
      "Epoch: 390, Cost: 1.484458898833168\n",
      "Epoch: 400, Cost: 1.48320909245082\n",
      "Epoch: 410, Cost: 1.4819995691426238\n",
      "Epoch: 420, Cost: 1.480790810316249\n",
      "Epoch: 430, Cost: 1.479610126356412\n",
      "Epoch: 440, Cost: 1.478728150250816\n",
      "Epoch: 450, Cost: 1.478408344890885\n",
      "Epoch: 460, Cost: 1.478387106902577\n",
      "Epoch: 470, Cost: 1.478343464381629\n",
      "Epoch: 480, Cost: 1.4782845652605807\n",
      "Epoch: 490, Cost: 1.4782018554723486\n",
      "Epoch: 500, Cost: 1.4780746316695255\n",
      "Epoch: 510, Cost: 1.4779276166423927\n",
      "Epoch: 520, Cost: 1.4779355129480722\n",
      "Epoch: 530, Cost: 1.4780910111576853\n",
      "Epoch: 540, Cost: 1.4782802399037578\n",
      "Epoch: 550, Cost: 1.4784010886763772\n",
      "Epoch: 560, Cost: 1.4785314988802565\n",
      "Epoch: 570, Cost: 1.478679175274554\n",
      "Epoch: 580, Cost: 1.4785167657035108\n",
      "Epoch: 590, Cost: 1.4783119694043836\n",
      "Epoch: 600, Cost: 1.4781047601974768\n",
      "Epoch: 610, Cost: 1.4779169700941859\n",
      "Epoch: 620, Cost: 1.4776833084606817\n",
      "Epoch: 630, Cost: 1.4778889850087245\n",
      "Epoch: 640, Cost: 1.477929745561671\n",
      "Epoch: 650, Cost: 1.4778627762586607\n",
      "Epoch: 660, Cost: 1.4778342142238619\n",
      "Epoch: 670, Cost: 1.4779132937655768\n",
      "Epoch: 680, Cost: 1.4779858063580638\n",
      "Epoch: 690, Cost: 1.4780267641201907\n",
      "Epoch: 700, Cost: 1.4780026546775105\n",
      "Epoch: 710, Cost: 1.4780415984477266\n",
      "Epoch: 720, Cost: 1.4781757314150965\n",
      "Epoch: 730, Cost: 1.4783409628721855\n",
      "Epoch: 740, Cost: 1.4786137703838413\n",
      "Epoch: 750, Cost: 1.4786872196918803\n",
      "Epoch: 760, Cost: 1.4787567156564627\n",
      "Epoch: 770, Cost: 1.4787136728188386\n",
      "Epoch: 780, Cost: 1.4788298617044575\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# currently 10k with 2 hidden layers and leaky relu takes 14-16 minutes (with my pc)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m weights, biases, num_hidden_layers, activation_function_ \u001b[38;5;241m=\u001b[39m feedforward(training_data, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m10000\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 59\u001b[0m, in \u001b[0;36mfeedforward\u001b[1;34m(data, num_hidden_layers, learning_rate, epochs, activation_function)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28minput\u001b[39m, output \u001b[38;5;241m=\u001b[39m forward_pass(data_no_labels, weights, biases, num_hidden_layers,activation_function_)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m---> 59\u001b[0m     weights, biases \u001b[38;5;241m=\u001b[39m backward_pass(data_no_labels, classes, weights, biases, \u001b[38;5;28minput\u001b[39m, output, activation_function_derivative_, cost_derivative_, learning_rate)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28minput\u001b[39m, output \u001b[38;5;241m=\u001b[39m forward_pass(data_no_labels, weights, biases, num_hidden_layers, activation_function_)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[4], line 50\u001b[0m, in \u001b[0;36mbackward_pass\u001b[1;34m(data, labels, weights, biases, inputs, outputs, activation_function_derivative, cost_derivative, learning_rate)\u001b[0m\n\u001b[0;32m     47\u001b[0m     biases[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden\u001b[39m\u001b[38;5;124m\"\u001b[39m][i] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m gradient_biases_hidden\n\u001b[0;32m     49\u001b[0m delta \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(delta, weights[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;241m*\u001b[39m activation_function_derivative(inputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 50\u001b[0m gradient_weights_input \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(data\u001b[38;5;241m.\u001b[39mT, delta)\n\u001b[0;32m     51\u001b[0m gradient_biases_input \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(delta, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Update input layer weights and biases\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# currently 10k with 2 hidden layers and leaky relu takes 14-16 minutes (with my pc)\n",
    "weights, biases, num_hidden_layers, activation_function_ = feedforward(training_data, 2, 0.01, 10000, 'relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.14600000000000002, 0.025000000000000022, 0.11699999999999999, 0.18400000000000005, 0.406]\n",
      "Accuracy: 82.44%\n",
      "[[854  16  25  27  78]\n",
      " [  4 975   4  15   2]\n",
      " [ 22  12 883   8  75]\n",
      " [ 57  58  29 816  40]\n",
      " [178  11 183  34 594]]\n"
     ]
    }
   ],
   "source": [
    "accuracy, confusion_matrix = predict(test_data, weights, biases, num_hidden_layers, activation_function_)\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  7  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ... 34  0  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\krist\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "MLPClassifier.predict() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 14\u001b[0m\n\u001b[0;32m     10\u001b[0m clf_MLP\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_test)\n\u001b[1;32m---> 14\u001b[0m clf_MLP\u001b[38;5;241m.\u001b[39mpredict(X_test,y_test)\n",
      "\u001b[1;31mTypeError\u001b[0m: MLPClassifier.predict() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "X_train = training_data[:,:-1]\n",
    "y_train = training_data[:,-1]\n",
    "X_test = test_data[:,:-1]\n",
    "y_test = test_data[:,-1]\n",
    "\n",
    "clf_MLP = MLPClassifier(random_state=42)\n",
    "clf_MLP.fit(X_train, y_train)\n",
    "\n",
    "print(X_test)\n",
    "\n",
    "clf_MLP.predict(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
