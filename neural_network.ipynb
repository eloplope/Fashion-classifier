{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.load(f'../fashion_train.npy')\n",
    "test_data = np.load(f'../fashion_test.npy')\n",
    "\n",
    "labels = training_data[:, -1]\n",
    "training_data = training_data[:,:-1] / 255 # Normalise pixel values\n",
    "training_data = np.c_[training_data, labels] # Add labels back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def MSE(y_pred, y):\n",
    "    return 2 * np.mean([(y_i - y_pred_i)**2 for y_i, y_pred_i in zip(y, y_pred)])\n",
    "\n",
    "def MSE_derivative(y_pred, y):\n",
    "    return [(y_i - y_pred_i) for y_i, y_pred_i in zip(y, y_pred)]\n",
    "\n",
    "def CrossEntropy(y_pred, y):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(np.sum(y * np.log(y_pred), axis=1))\n",
    "\n",
    "def CrossEntropy_derivative(y_pred, y):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return (y_pred - y) / y.shape[0]\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def reLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def reLU_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def leaky_reLU(x):\n",
    "    return np.maximum(0.01 * x, x)\n",
    "\n",
    "def leaky_reLU_derivative(x):\n",
    "    return np.where(x > 0, 1, 0.01)\n",
    "\n",
    "def ELU(x):\n",
    "    return np.maximum(0.01 * (np.exp(x)-1), x)\n",
    "\n",
    "def ELU_derivative(x):\n",
    "    return np.where(x > 0, 1, 0.01* np.exp(x))\n",
    "\n",
    "def softmax(x):\n",
    "    exp_values = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "    return probabilities\n",
    "\n",
    "def xavier_initialization(input_size, output_size):\n",
    "    return np.random.randn(input_size, output_size) * np.sqrt(1 / input_size)\n",
    "\n",
    "def he_initialization(input_size, output_size):\n",
    "    return np.random.randn(input_size, output_size) * np.sqrt(2 / input_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(data, weights, biases, num_hidden_layers, activation_function):\n",
    "    inputs, outputs = [], []\n",
    "    current_input = data\n",
    "\n",
    "    z = np.dot(current_input, weights[\"input\"]) + biases[\"input\"]\n",
    "    inputs.append(z)\n",
    "    current_input = activation_function(z)\n",
    "    outputs.append(current_input)\n",
    "\n",
    "    for i in range(num_hidden_layers):\n",
    "        z = np.dot(current_input, weights[\"hidden\"][i]) + biases[\"hidden\"][i]\n",
    "        inputs.append(z)\n",
    "        current_input = activation_function(z)\n",
    "        outputs.append(current_input)\n",
    "\n",
    "    z = np.dot(current_input, weights[\"output\"]) + biases[\"output\"]\n",
    "    inputs.append(z)\n",
    "    current_input = softmax(z)\n",
    "    outputs.append(current_input)\n",
    "\n",
    "    return inputs, outputs\n",
    "\n",
    "def backward_pass(data, labels, weights, biases, inputs, outputs, activation_function_derivative, cost_derivative, learning_rate):\n",
    "    # Calculate delta for output layer\n",
    "    delta = cost_derivative(outputs[-1], labels) * activation_function_derivative(inputs[-1])\n",
    "    \n",
    "    # Gradients for output layer\n",
    "    gradient_weights_output = np.dot(outputs[-2].T, delta)  # Last hidden layer's output\n",
    "    gradient_biases_output = np.sum(delta, axis=0, keepdims=True)\n",
    "\n",
    "    # Update output layer weights and biases\n",
    "    weights[\"output\"] -= learning_rate * gradient_weights_output\n",
    "    biases[\"output\"] -= learning_rate * gradient_biases_output\n",
    "\n",
    "    num_hidden_layers = len(weights[\"hidden\"])\n",
    "    # Gradients for hidden layers\n",
    "    for i in range(num_hidden_layers-1, -1, -1):\n",
    "        if i == len(weights[\"hidden\"])-1:\n",
    "            delta = np.dot(delta, weights[\"output\"].T) * activation_function_derivative(inputs[-2])\n",
    "        else:\n",
    "            delta = np.dot(delta, weights[\"hidden\"][i+1].T) * activation_function_derivative(inputs[-2 +  i - num_hidden_layers])\n",
    "\n",
    "        gradient_weights_hidden = np.dot(outputs[-2 +  i - num_hidden_layers].T, delta)\n",
    "        gradient_biases_hidden = np.sum(delta, axis=0, keepdims=True)\n",
    "        # Update hidden layer weights and biases\n",
    "        weights[\"hidden\"][i] -= learning_rate * gradient_weights_hidden\n",
    "        biases[\"hidden\"][i] -= learning_rate * gradient_biases_hidden\n",
    "\n",
    "    delta = np.dot(delta, weights[\"hidden\"][0].T) * activation_function_derivative(inputs[0])\n",
    "    gradient_weights_input = np.dot(data.T, delta)\n",
    "    gradient_biases_input = np.sum(delta, axis=0, keepdims=True)\n",
    "\n",
    "    # Update input layer weights and biases\n",
    "    weights[\"input\"] -= learning_rate * gradient_weights_input\n",
    "    biases[\"input\"] -= learning_rate * gradient_biases_input\n",
    "\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(data, num_hidden_layers, learning_rate, epochs, activation_function):\n",
    "    np.random.seed(42)\n",
    "\n",
    "    if activation_function == 'sigmoid':\n",
    "        initialization = xavier_initialization\n",
    "        activation_function_ = sigmoid\n",
    "        activation_function_derivative_ = sigmoid_derivative\n",
    "        cost_function_ = MSE\n",
    "        cost_derivative_ = MSE_derivative\n",
    "    elif activation_function == 'relu':\n",
    "        initialization = he_initialization\n",
    "        activation_function_ = reLU\n",
    "        activation_function_derivative_ = reLU_derivative\n",
    "        cost_function_ = CrossEntropy\n",
    "        cost_derivative_ = CrossEntropy_derivative\n",
    "    elif activation_function == 'leaky_relu':\n",
    "        initialization = he_initialization\n",
    "        activation_function_ = leaky_reLU\n",
    "        activation_function_derivative_ = leaky_reLU_derivative\n",
    "        cost_function_ = CrossEntropy\n",
    "        cost_derivative_ = CrossEntropy_derivative\n",
    "    elif activation_function == 'ELU':\n",
    "        initialization = he_initialization\n",
    "        activation_function_ = ELU\n",
    "        activation_function_derivative_ = ELU_derivative\n",
    "        cost_function_ = CrossEntropy\n",
    "        cost_derivative_ = CrossEntropy_derivative\n",
    "    else:\n",
    "        raise ValueError(\"Invalid activation function. Choose 'sigmoid', 'leaky_relu', 'ELU' or 'relu'.\")\n",
    "        \n",
    "    # Weight initialization\n",
    "    num_samples, num_features = data.shape[0], data.shape[1]-1\n",
    "    input_layer_size = num_features\n",
    "    hidden_layer_size = 16\n",
    "\n",
    "    output_layer_size = len(np.unique(data[:,-1]))\n",
    "\n",
    "    weights = {\n",
    "        \"input\": initialization(input_layer_size, hidden_layer_size),\n",
    "        \"hidden\": np.array([initialization(hidden_layer_size, hidden_layer_size) for _ in range(num_hidden_layers)]),\n",
    "        \"output\": initialization(hidden_layer_size, output_layer_size)\n",
    "    }\n",
    "    biases = {\n",
    "        \"input\": np.zeros((1, hidden_layer_size)),\n",
    "        \"hidden\": [np.zeros((1, hidden_layer_size)) for _ in range(num_hidden_layers)],\n",
    "        \"output\": np.zeros((1, output_layer_size))\n",
    "    }\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    labels = data[:, -1]\n",
    "    data_no_labels = data[:, :-1]\n",
    "    num_classes = len(np.unique(labels))\n",
    "    classes = np.eye(num_classes)[labels.astype(int)]\n",
    "\n",
    "    input, output = forward_pass(data_no_labels, weights, biases, num_hidden_layers,activation_function_)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        weights, biases = backward_pass(data_no_labels, classes, weights, biases, input, output, activation_function_derivative_, cost_derivative_, learning_rate)\n",
    "\n",
    "        input, output = forward_pass(data_no_labels, weights, biases, num_hidden_layers, activation_function_)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            cost = cost_function_(output[-1], classes)\n",
    "            print(f\"Epoch: {epoch}, Cost: {cost}\")\n",
    "    return weights, biases, num_hidden_layers, activation_function_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, weights, biases, num_hidden_layers, activation_function):\n",
    "    data_labels = data[:, -1].astype(int)\n",
    "    data_no_labels = data[:, :-1]\n",
    "    inputs, outputs = forward_pass(data_no_labels, weights, biases, num_hidden_layers, activation_function)\n",
    "    pred = np.argmax(outputs[-1], axis=1).astype(int)\n",
    "    accuracy = (np.sum(pred == data_labels) / len(data_labels)) * 100\n",
    "    num_classes = len(np.unique(data_labels))\n",
    "    confusion_matrix = np.zeros((num_classes, num_classes),dtype=int)\n",
    "    for i in range(len(data_labels)):\n",
    "        confusion_matrix[data_labels[i], pred[i]] += 1\n",
    "    \n",
    "    misclassification_rate = []\n",
    "    for i in range(num_classes):\n",
    "        total_row = np.sum(confusion_matrix[i,:])\n",
    "        error_rate = confusion_matrix[i,i] / total_row\n",
    "        misclassification_rate.append(1-error_rate)\n",
    "    print(misclassification_rate)\n",
    "    return accuracy, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cost: 1.575884441435344\n",
      "Epoch: 10, Cost: 1.5002745434583566\n",
      "Epoch: 20, Cost: 1.450113831093218\n",
      "Epoch: 30, Cost: 1.4092800240789565\n",
      "Epoch: 40, Cost: 1.372334543726602\n",
      "Epoch: 50, Cost: 1.338769435669835\n",
      "Epoch: 60, Cost: 1.3077073164170463\n",
      "Epoch: 70, Cost: 1.2792491084460282\n",
      "Epoch: 80, Cost: 1.253113769738373\n",
      "Epoch: 90, Cost: 1.228975152109408\n",
      "Epoch: 100, Cost: 1.2066608321123922\n",
      "Epoch: 110, Cost: 1.1860454200444994\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# currently 10k with 2 hidden layers and leaky relu takes 14-16 minutes (with my pc)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m weights, biases, num_hidden_layers, activation_function_ \u001b[38;5;241m=\u001b[39m feedforward(training_data, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m2000\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 61\u001b[0m, in \u001b[0;36mfeedforward\u001b[1;34m(data, num_hidden_layers, learning_rate, epochs, activation_function)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     59\u001b[0m     weights, biases \u001b[38;5;241m=\u001b[39m backward_pass(data_no_labels, classes, weights, biases, \u001b[38;5;28minput\u001b[39m, output, activation_function_derivative_, cost_derivative_, learning_rate)\n\u001b[1;32m---> 61\u001b[0m     \u001b[38;5;28minput\u001b[39m, output \u001b[38;5;241m=\u001b[39m forward_pass(data_no_labels, weights, biases, num_hidden_layers, activation_function_)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     64\u001b[0m         cost \u001b[38;5;241m=\u001b[39m cost_function_(output[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], classes)\n",
      "Cell \u001b[1;32mIn[10], line 5\u001b[0m, in \u001b[0;36mforward_pass\u001b[1;34m(data, weights, biases, num_hidden_layers, activation_function)\u001b[0m\n\u001b[0;32m      2\u001b[0m inputs, outputs \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m      3\u001b[0m current_input \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m----> 5\u001b[0m z \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(current_input, weights[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m+\u001b[39m biases[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      6\u001b[0m inputs\u001b[38;5;241m.\u001b[39mappend(z)\n\u001b[0;32m      7\u001b[0m current_input \u001b[38;5;241m=\u001b[39m activation_function(z)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# currently 10k with 2 hidden layers and leaky relu takes 14-16 minutes (with my pc)\n",
    "weights, biases, num_hidden_layers, activation_function_ = feedforward(training_data, 1, 0.01, 2000, 'relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.18100000000000005, 0.050000000000000044, 0.11299999999999999, 0.135, 0.40900000000000003]\n",
      "Accuracy: 82.24%\n",
      "[[819   3  32  48  98]\n",
      " [  3 950  15  30   2]\n",
      " [ 19   3 887  14  77]\n",
      " [ 38  22  24 865  51]\n",
      " [159   3 207  40 591]]\n"
     ]
    }
   ],
   "source": [
    "accuracy, confusion_matrix = predict(test_data, weights, biases, num_hidden_layers, activation_function_)\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  7  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ... 34  0  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\krist\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "MLPClassifier.predict() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 14\u001b[0m\n\u001b[0;32m     10\u001b[0m clf_MLP\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_test)\n\u001b[1;32m---> 14\u001b[0m clf_MLP\u001b[38;5;241m.\u001b[39mpredict(X_test,y_test)\n",
      "\u001b[1;31mTypeError\u001b[0m: MLPClassifier.predict() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "X_train = training_data[:,:-1]\n",
    "y_train = training_data[:,-1]\n",
    "X_test = test_data[:,:-1]\n",
    "y_test = test_data[:,-1]\n",
    "\n",
    "clf_MLP = MLPClassifier(random_state=42)\n",
    "clf_MLP.fit(X_train, y_train)\n",
    "\n",
    "print(X_test)\n",
    "\n",
    "clf_MLP.predict(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
