{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3., 4.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = np.load(f'../fashion_train.npy')\n",
    "test_data = np.load(f'../fashion_test.npy')\n",
    "\n",
    "labels = training_data[:, -1]\n",
    "training_data = training_data[:,:-1] / 255\n",
    "training_data = np.c_[training_data, labels]\n",
    "\n",
    "np.unique(training_data[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 1.6978854635849696\n",
      "epoch: 1, loss: 1.759686982825285\n",
      "epoch: 2, loss: 1.7130786893125773\n",
      "epoch: 3, loss: 1.6654278383523982\n",
      "epoch: 4, loss: 1.6572890669560092\n",
      "epoch: 5, loss: 1.634593310154228\n",
      "epoch: 6, loss: 1.6358336232516861\n",
      "epoch: 7, loss: 1.6278669094735219\n",
      "epoch: 8, loss: 1.6252554738903593\n",
      "epoch: 9, loss: 1.621416312308237\n",
      "epoch: 10, loss: 1.6185758006194435\n",
      "epoch: 11, loss: 1.6150030687745551\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 189\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(cost)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m current_output\n\u001b[1;32m--> 189\u001b[0m tmp \u001b[38;5;241m=\u001b[39m feedforward(training_data, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0.000005\u001b[39m, \u001b[38;5;241m200\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleaky_relu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 176\u001b[0m, in \u001b[0;36mfeedforward\u001b[1;34m(data, num_hidden_layers, learning_rate, epochs, activation_function)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    174\u001b[0m     hidden_layer_inputs[i] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(hidden_layer_inputs[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], hidden_layer_weights[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m+\u001b[39m hidden_layer_bias[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 176\u001b[0m hidden_layer_outputs[i] \u001b[38;5;241m=\u001b[39m activation_function_(hidden_layer_inputs[i])\n\u001b[0;32m    178\u001b[0m output_layer_input \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(hidden_layer_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], output_layer_weights) \u001b[38;5;241m+\u001b[39m output_layer_bias\n\u001b[0;32m    179\u001b[0m output_layer_output \u001b[38;5;241m=\u001b[39m softmax(output_layer_input)\n",
      "Cell \u001b[1;32mIn[15], line 31\u001b[0m, in \u001b[0;36mleaky_reLU\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreLU_derivative\u001b[39m(x):\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mwhere(x \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mleaky_reLU\u001b[39m(x):\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmaximum(\u001b[38;5;241m0.01\u001b[39m \u001b[38;5;241m*\u001b[39m x, x)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mleaky_reLU_derivative\u001b[39m(x):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Helper functions\n",
    "def MSE(y_pred, y):\n",
    "    return np.mean([(y_i - y_pred_i)**2 for y_i, y_pred_i in zip(y, y_pred)])\n",
    "\n",
    "def MSE_derivative(y_pred, y):\n",
    "    return [2 * (y_i - y_pred_i) for y_i, y_pred_i in zip(y, y_pred)]\n",
    "\n",
    "def CrossEntropy(y_pred, y):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(np.sum(y * np.log(y_pred), axis=1))\n",
    "\n",
    "def CrossEntropy_derivative(y_pred, y):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return y_pred - y\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def reLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def reLU_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def leaky_reLU(x):\n",
    "    return np.maximum(0.01 * x, x)\n",
    "\n",
    "def leaky_reLU_derivative(x):\n",
    "    return np.where(x > 0, 1, 0.01)\n",
    "\n",
    "def ELU(x):\n",
    "    return np.maximum(0.01 * (np.exp(x)-1), x)\n",
    "\n",
    "def ELU_derivative(x):\n",
    "    return np.where(x > 0, 1, 0.01* np.exp(x))\n",
    "\n",
    "def softmax(x):\n",
    "    exp_values = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "    return probabilities\n",
    "\n",
    "def xavier_initialization(input_size, output_size):\n",
    "    return np.random.randn(input_size, output_size) * np.sqrt(1 / input_size)\n",
    "\n",
    "def he_initialization(input_size, output_size):\n",
    "    return np.random.randn(input_size, output_size) * np.sqrt(2 / input_size)\n",
    "\n",
    "\n",
    "def feedforward(data, num_hidden_layers, learning_rate, epochs, activation_function):\n",
    "\n",
    "    # Select activation function\n",
    "    if activation_function == 'sigmoid':\n",
    "        initialization = xavier_initialization\n",
    "        activation_function_ = sigmoid\n",
    "        activation_function_derivative_ = sigmoid_derivative\n",
    "        cost_function_ = MSE\n",
    "        cost_derivative_ = MSE_derivative\n",
    "    elif activation_function == 'relu':\n",
    "        initialization = he_initialization\n",
    "        activation_function_ = reLU\n",
    "        activation_function_derivative_ = reLU_derivative\n",
    "        cost_function_ = CrossEntropy\n",
    "        cost_derivative_ = CrossEntropy_derivative\n",
    "    elif activation_function == 'leaky_relu':\n",
    "        initialization = he_initialization\n",
    "        activation_function_ = leaky_reLU\n",
    "        activation_function_derivative_ = leaky_reLU_derivative\n",
    "        cost_function_ = CrossEntropy\n",
    "        cost_derivative_ = CrossEntropy_derivative\n",
    "    elif activation_function == 'ELU':\n",
    "        initialization = he_initialization\n",
    "        activation_function_ = ELU\n",
    "        activation_function_derivative_ = ELU_derivative\n",
    "        cost_function_ = CrossEntropy\n",
    "        cost_derivative_ = CrossEntropy_derivative\n",
    "    else:\n",
    "        raise ValueError(\"Invalid activation function. Choose 'sigmoid', 'leaky_relu', 'ELU' or 'relu'.\")\n",
    "        \n",
    "    # Weight initialization\n",
    "    num_samples, num_features = data.shape[0], data.shape[1]-1\n",
    "    input_layer_size = num_features\n",
    "    hidden_layer_size = num_features // num_hidden_layers\n",
    "\n",
    "    input_layer_weights = initialization(input_layer_size, hidden_layer_size)\n",
    "    input_layer_bias = np.zeros((1, hidden_layer_size))\n",
    "\n",
    "    hidden_layer_weights = [initialization(hidden_layer_size, hidden_layer_size) for _ in range(num_hidden_layers)]\n",
    "    hidden_layer_bias = [np.zeros((1, hidden_layer_size)) for _ in range(num_hidden_layers)]\n",
    "\n",
    "    output_layer_size = len(np.unique(data[:,-1]))\n",
    "    output_layer_weights = initialization(hidden_layer_size, output_layer_size)\n",
    "    output_layer_bias = np.zeros((1, output_layer_size))\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    labels = data[:, -1]\n",
    "    num_classes = len(np.unique(labels))\n",
    "    classes = np.eye(num_classes)[labels.astype(int)]\n",
    "\n",
    "    hidden_layer_inputs = [None] * num_hidden_layers\n",
    "    hidden_layer_outputs = [None] * num_hidden_layers\n",
    "\n",
    "    current_output = np.zeros((num_samples, output_layer_size))\n",
    "\n",
    "    # first forward pass\n",
    "    for i in range(num_hidden_layers):\n",
    "        if i == 0:\n",
    "            hidden_layer_inputs[i] = np.dot(data[:,:-1], input_layer_weights) + input_layer_bias\n",
    "        else:\n",
    "            hidden_layer_inputs[i] = np.dot(hidden_layer_outputs[i-1], hidden_layer_weights[i-1]) + hidden_layer_bias[i-1]\n",
    "\n",
    "        hidden_layer_outputs[i] = activation_function_(hidden_layer_inputs[i])\n",
    "\n",
    "\n",
    "    output_layer_input = np.dot(hidden_layer_outputs[-1], output_layer_weights) + output_layer_bias\n",
    "\n",
    "    # Activation function for the output should be softmax becasue we have multiple classes\n",
    "    output_layer_output = softmax(output_layer_input)\n",
    "    current_output = output_layer_output\n",
    "    # -------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Backpropagation\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Compute cost\n",
    "        cost = cost_function_(current_output, classes)\n",
    "        \n",
    "        # Weight and bias updates for the output layer\n",
    "        delta = cost_derivative_(current_output, classes) * activation_function_derivative_(output_layer_input)\n",
    "        gradient_output_layer_weights = np.dot(hidden_layer_outputs[-1].T, delta)\n",
    "        gradient_output_layer_bias = np.sum(delta, axis=0, keepdims=True)\n",
    "\n",
    "        output_layer_weights -= learning_rate * gradient_output_layer_weights\n",
    "        output_layer_bias -= learning_rate * gradient_output_layer_bias\n",
    "\n",
    "        # Weight and bias updates for hidden layers\n",
    "        for i in range(num_hidden_layers-1, -1, -1):\n",
    "            if i == num_hidden_layers-1:\n",
    "                # For the last hidden layer\n",
    "                delta = np.dot(delta, output_layer_weights.T) * activation_function_derivative_(hidden_layer_inputs[i])\n",
    "            else:\n",
    "                # For other hidden layers\n",
    "                delta = np.dot(delta, hidden_layer_weights[i+1].T) * activation_function_derivative_(hidden_layer_inputs[i])\n",
    "            \n",
    "            # Compute gradients\n",
    "            if i > 0:\n",
    "                # For layers with a previous hidden layer\n",
    "                gradient_hidden_layer_weights = np.dot(hidden_layer_outputs[i-1].T, delta)\n",
    "            else:\n",
    "                # For the first hidden layer (connected to input layer)\n",
    "                gradient_hidden_layer_weights = np.dot(data[:,:-1].T, delta)\n",
    "            \n",
    "            gradient_hidden_layer_bias = np.sum(delta, axis=0, keepdims=True)\n",
    "            \n",
    "            # Update weights and biases\n",
    "            if i > 0:\n",
    "                hidden_layer_weights[i] -= learning_rate * gradient_hidden_layer_weights\n",
    "                hidden_layer_bias[i] -= learning_rate * gradient_hidden_layer_bias\n",
    "            else:\n",
    "                input_layer_weights -= learning_rate * gradient_hidden_layer_weights\n",
    "                input_layer_bias -= learning_rate * gradient_hidden_layer_bias\n",
    "\n",
    "        # Forward pass\n",
    "        for i in range(num_hidden_layers):\n",
    "            if i == 0:\n",
    "                hidden_layer_inputs[i] = np.dot(data[:,:-1], input_layer_weights) + input_layer_bias\n",
    "            else:\n",
    "                hidden_layer_inputs[i] = np.dot(hidden_layer_inputs[i-1], hidden_layer_weights[i-1]) + hidden_layer_bias[i-1]\n",
    "\n",
    "            hidden_layer_outputs[i] = activation_function_(hidden_layer_inputs[i])\n",
    "\n",
    "            output_layer_input = np.dot(hidden_layer_outputs[-1], output_layer_weights) + output_layer_bias\n",
    "            output_layer_output = softmax(output_layer_input)\n",
    "            \n",
    "        current_output = output_layer_output\n",
    "\n",
    "        # if epoch % 10 == 0:\n",
    "        print(f'epoch: {epoch}, loss: {np.mean(cost)}')\n",
    "\n",
    "\n",
    "    return current_output\n",
    "\n",
    "tmp = feedforward(training_data, 2, 0.000005, 200, 'leaky_relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(data, weights, biases, activation_function):\n",
    "    inputs, outputs = [], []\n",
    "    current_input = data\n",
    "\n",
    "    for w, b in zip(weights.values(), biases.values()):\n",
    "        print(\"w_shape\", w.shape)\n",
    "        print(\"current_input_shape\", current_input.shape)\n",
    "        z = np.dot(current_input, w) + b\n",
    "        print(\"z_shape\", z.shape)\n",
    "        inputs.append(z)\n",
    "        current_input = activation_function(z)\n",
    "        outputs.append(current_input)\n",
    "    \n",
    "    return inputs, outputs\n",
    "\n",
    "\n",
    "def backward_pass(data, labels, weights, biases, inputs, outputs, activation_function_derivative, cost_derivative, learning_rate):\n",
    "    print(\"output shape\", outputs[-1].shape)\n",
    "    print(\"labels shape\", labels.shape)\n",
    "    print(\"inputs shape\", inputs[-1].shape)\n",
    "    print(outputs)\n",
    "    delta = cost_derivative(outputs[-1], labels) * activation_function_derivative(inputs[-1])\n",
    "    \n",
    "    for i in range(len(weights)):\n",
    "        if weights[i].keys() != \"hidden\":\n",
    "            delta = np.dot(delta, weights[i].T if i > 0 else data[:,:-1].T) * activation_function_derivative(inputs[i])\n",
    "            gradient_weights = np.dot(outputs[i-1].T, delta)\n",
    "            gradient_biases = np.sum(delta, axis=0, keepdims=True)\n",
    "\n",
    "            weights[i] -= learning_rate * gradient_weights\n",
    "            biases[i] -= learning_rate * gradient_biases\n",
    "        else:\n",
    "            for k in range(len(weights[i])):\n",
    "                delta = np.dot(delta, weights[i][k].T) * activation_function_derivative(inputs[i][k])\n",
    "                gradient_weights = np.dot(outputs[i-1].T, delta)\n",
    "                gradient_biases = np.sum(delta, axis=0, keepdims=True)\n",
    "\n",
    "                weights[i][k] -= learning_rate * gradient_weights\n",
    "                biases[i][k] -= learning_rate * gradient_biases\n",
    "   \n",
    "    return weights, biases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_shape (784, 392)\n",
      "current_input_shape (10000, 784)\n",
      "z_shape (10000, 392)\n",
      "w_shape (2, 392, 392)\n",
      "current_input_shape (10000, 392)\n",
      "z_shape (10000, 2, 392)\n",
      "w_shape (392, 5)\n",
      "current_input_shape (10000, 2, 392)\n",
      "z_shape (10000, 2, 5)\n",
      "output shape (10000, 2, 5)\n",
      "labels shape (10000, 5)\n",
      "inputs shape (10000, 2, 5)\n",
      "[array([[0.22539573, 0.        , 0.95737522, ..., 0.38700935, 0.        ,\n",
      "        0.41582177],\n",
      "       [0.13308202, 0.26426893, 0.50084099, ..., 0.2174232 , 0.        ,\n",
      "        0.17742599],\n",
      "       [0.44477333, 0.        , 0.92245097, ..., 0.52439474, 0.        ,\n",
      "        0.49124787],\n",
      "       ...,\n",
      "       [0.        , 0.22537091, 0.6318817 , ..., 0.56941705, 0.        ,\n",
      "        0.88111488],\n",
      "       [0.52715771, 0.        , 0.51151227, ..., 0.35362733, 0.        ,\n",
      "        0.51339508],\n",
      "       [0.        , 0.        , 0.54485478, ..., 0.40421756, 0.        ,\n",
      "        0.5240811 ]]), array([[[0.        , 0.92491724, 0.62100167, ..., 0.        ,\n",
      "         0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.49285199, ..., 0.        ,\n",
      "         0.00274474, 0.04642874]],\n",
      "\n",
      "       [[0.05611515, 0.03229743, 0.47020284, ..., 0.        ,\n",
      "         0.        , 0.        ],\n",
      "        [0.        , 0.10179624, 0.61724193, ..., 0.        ,\n",
      "         0.0544845 , 0.04097008]],\n",
      "\n",
      "       [[0.        , 1.11820599, 0.58973415, ..., 0.        ,\n",
      "         0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.60791559, ..., 0.        ,\n",
      "         0.        , 0.09030533]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0.        , 0.30415471, 1.28977283, ..., 0.        ,\n",
      "         0.        , 0.        ],\n",
      "        [0.        , 0.45098538, 0.92903618, ..., 0.        ,\n",
      "         0.43101514, 0.        ]],\n",
      "\n",
      "       [[0.        , 1.13306766, 0.67640698, ..., 0.        ,\n",
      "         0.        , 0.        ],\n",
      "        [0.        , 0.19360041, 0.31409661, ..., 0.        ,\n",
      "         0.        , 0.        ]],\n",
      "\n",
      "       [[0.        , 0.        , 0.03780124, ..., 0.        ,\n",
      "         0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.56632404, ..., 0.        ,\n",
      "         0.3005357 , 0.10230582]]]), array([[[0.41589033, 0.49077475, 0.11581692, 0.69002418, 0.        ],\n",
      "        [0.29647332, 0.        , 0.        , 0.77232289, 0.        ]],\n",
      "\n",
      "       [[0.52671759, 0.00108983, 0.55279399, 0.70075523, 0.        ],\n",
      "        [0.34560684, 0.08028412, 0.        , 0.35657722, 0.        ]],\n",
      "\n",
      "       [[0.62990922, 0.65251255, 0.03744654, 0.72310037, 0.        ],\n",
      "        [0.52011378, 0.        , 0.        , 1.02554078, 0.        ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0.62182894, 0.3743285 , 0.68778299, 0.84693311, 0.        ],\n",
      "        [0.80862883, 0.        , 0.        , 0.69093345, 0.        ]],\n",
      "\n",
      "       [[0.28375814, 0.75053131, 0.32270552, 0.64574949, 0.        ],\n",
      "        [0.43185539, 0.        , 0.        , 0.8267421 , 0.        ]],\n",
      "\n",
      "       [[0.33101603, 0.49834835, 0.44741148, 1.007817  , 0.        ],\n",
      "        [0.89583364, 0.03468092, 0.        , 0.52104351, 0.        ]]])]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10000,2,5) (10000,5) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 115\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28mprint\u001b[39m(weights)\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m weights\n\u001b[1;32m--> 115\u001b[0m feedforward2(training_data, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0.000001\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[19], line 110\u001b[0m, in \u001b[0;36mfeedforward2\u001b[1;34m(data, num_hidden_layers, learning_rate, epochs, activation_function)\u001b[0m\n\u001b[0;32m    107\u001b[0m classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39meye(num_classes)[labels\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)]\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28minput\u001b[39m, output \u001b[38;5;241m=\u001b[39m forward_pass(data[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], weights, biases, activation_function_)\n\u001b[1;32m--> 110\u001b[0m weights, biases \u001b[38;5;241m=\u001b[39m backward_pass(data, classes, weights, biases, \u001b[38;5;28minput\u001b[39m, output, activation_function_derivative_, cost_derivative_, learning_rate)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mprint\u001b[39m(weights)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m weights\n",
      "Cell \u001b[1;32mIn[18], line 22\u001b[0m, in \u001b[0;36mbackward_pass\u001b[1;34m(data, labels, weights, biases, inputs, outputs, activation_function_derivative, cost_derivative, learning_rate)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs shape\u001b[39m\u001b[38;5;124m\"\u001b[39m, inputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs)\n\u001b[1;32m---> 22\u001b[0m delta \u001b[38;5;241m=\u001b[39m cost_derivative(outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], labels) \u001b[38;5;241m*\u001b[39m activation_function_derivative(inputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(weights)):\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m weights[i]\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "Cell \u001b[1;32mIn[19], line 16\u001b[0m, in \u001b[0;36mCrossEntropy_derivative\u001b[1;34m(y_pred, y)\u001b[0m\n\u001b[0;32m     14\u001b[0m epsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-15\u001b[39m\n\u001b[0;32m     15\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(y_pred, epsilon, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m epsilon)\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y_pred \u001b[38;5;241m-\u001b[39m y\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10000,2,5) (10000,5) "
     ]
    }
   ],
   "source": [
    "# Helper functions\n",
    "def MSE(y_pred, y):\n",
    "    return 2 * np.mean([(y_i - y_pred_i)**2 for y_i, y_pred_i in zip(y, y_pred)])\n",
    "\n",
    "def MSE_derivative(y_pred, y):\n",
    "    return [(y_i - y_pred_i) for y_i, y_pred_i in zip(y, y_pred)]\n",
    "\n",
    "def CrossEntropy(y_pred, y):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(np.sum(y * np.log(y_pred), axis=1))\n",
    "\n",
    "def CrossEntropy_derivative(y_pred, y):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return y_pred - y\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def reLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def reLU_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def leaky_reLU(x):\n",
    "    return np.maximum(0.01 * x, x)\n",
    "\n",
    "def leaky_reLU_derivative(x):\n",
    "    return np.where(x > 0, 1, 0.01)\n",
    "\n",
    "def ELU(x):\n",
    "    return np.maximum(0.01 * (np.exp(x)-1), x)\n",
    "\n",
    "def ELU_derivative(x):\n",
    "    return np.where(x > 0, 1, 0.01* np.exp(x))\n",
    "\n",
    "def softmax(x):\n",
    "    exp_values = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "    return probabilities\n",
    "\n",
    "def xavier_initialization(input_size, output_size):\n",
    "    return np.random.randn(input_size, output_size) * np.sqrt(1 / input_size)\n",
    "\n",
    "def he_initialization(input_size, output_size):\n",
    "    return np.random.randn(input_size, output_size) * np.sqrt(2 / input_size)\n",
    "\n",
    "\n",
    "def feedforward2(data, num_hidden_layers, learning_rate, epochs, activation_function):\n",
    "\n",
    "    # Select activation function\n",
    "    if activation_function == 'sigmoid':\n",
    "        initialization = xavier_initialization\n",
    "        activation_function_ = sigmoid\n",
    "        activation_function_derivative_ = sigmoid_derivative\n",
    "        cost_function_ = MSE\n",
    "        cost_derivative_ = MSE_derivative\n",
    "    elif activation_function == 'relu':\n",
    "        initialization = he_initialization\n",
    "        activation_function_ = reLU\n",
    "        activation_function_derivative_ = reLU_derivative\n",
    "        cost_function_ = CrossEntropy\n",
    "        cost_derivative_ = CrossEntropy_derivative\n",
    "    elif activation_function == 'leaky_relu':\n",
    "        initialization = he_initialization\n",
    "        activation_function_ = leaky_reLU\n",
    "        activation_function_derivative_ = leaky_reLU_derivative\n",
    "        cost_function_ = CrossEntropy\n",
    "        cost_derivative_ = CrossEntropy_derivative\n",
    "    elif activation_function == 'ELU':\n",
    "        initialization = he_initialization\n",
    "        activation_function_ = ELU\n",
    "        activation_function_derivative_ = ELU_derivative\n",
    "        cost_function_ = CrossEntropy\n",
    "        cost_derivative_ = CrossEntropy_derivative\n",
    "    else:\n",
    "        raise ValueError(\"Invalid activation function. Choose 'sigmoid', 'leaky_relu', 'ELU' or 'relu'.\")\n",
    "        \n",
    "    # Weight initialization\n",
    "    num_samples, num_features = data.shape[0], data.shape[1]-1\n",
    "    input_layer_size = num_features\n",
    "    hidden_layer_size = num_features // num_hidden_layers\n",
    "\n",
    "    output_layer_size = len(np.unique(data[:,-1]))\n",
    "\n",
    "    weights = {\n",
    "        \"input\": initialization(input_layer_size, hidden_layer_size),\n",
    "        \"hidden\": np.array([initialization(hidden_layer_size, hidden_layer_size) for _ in range(num_hidden_layers)]),\n",
    "        \"output\": initialization(hidden_layer_size, output_layer_size)\n",
    "    }\n",
    "    biases = {\n",
    "        \"input\": np.zeros((1, hidden_layer_size)),\n",
    "        \"hidden\": np.zeros((1, hidden_layer_size)),\n",
    "        \"output\": np.zeros((1, output_layer_size))\n",
    "    }\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    labels = data[:, -1]\n",
    "    num_classes = len(np.unique(labels))\n",
    "    classes = np.eye(num_classes)[labels.astype(int)]\n",
    "    input, output = forward_pass(data[:, :-1], weights, biases, activation_function_)\n",
    "  \n",
    "    weights, biases = backward_pass(data, classes, weights, biases, input, output, activation_function_derivative_, cost_derivative_, learning_rate)\n",
    "\n",
    "    print(weights)\n",
    "    return weights\n",
    "\n",
    "feedforward2(training_data, 2, 0.000001, 3, 'relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
